volumes:
  n8n_storage:
    driver: local
  open-webui:
    driver: local
  traefik-certs:
    driver: local
  nocodb:
    driver: local
  postgres_data:
    driver: local
  backup_data:
    driver: local
  n8n-certs:
    driver: local
  whisperx-cache:
    driver: local
  registry-cache:
    driver: local
  shorts-generator-venv:
    driver: local
  # Shared AI model caches - reduces downloads across services
  hf-cache:
    driver: local
  torch-cache:
    driver: local
  pip-cache:
    driver: local
  comfyui-models:
    driver: local
  # Virtual Assistant volumes
  va-riva-cache:
    driver: local
  va-riva-tts-cache:
    driver: local
  va-audio2face-data:
    driver: local
  va-audio2face-models:
    driver: local
  # Ovi volumes - Audio-Video Generation Service
  ovi-models:
    driver: local
  ovi-outputs:
    driver: local
  # Wan2GP volumes - Multi-Model Video Generation Platform
  wan2gp-models:
    driver: local
  wan2gp-outputs:
    driver: local
  wan2gp-settings:
    driver: local
  wan2gp-loras:
    driver: local
  wan2gp-config:
    driver: local
  # InfiniteTalk volumes - Audio-Driven Video Dubbing
  # Emu3 volumes - Multimodal Image Generation
  emu3-models:
    driver: local

x-n8n: &service-n8n
  # Use pre-built image for faster startup (no rebuild on every start)
  image: localai-n8n:latest
  # Uncomment below to rebuild when Dockerfile changes:
  # build:
  #   context: ./n8n/docker
  #   dockerfile: Dockerfile
  environment:
    - DB_TYPE=postgresdb
    - DB_POSTGRESDB_HOST=${POSTGRES_HOST}
    - DB_POSTGRESDB_PORT=${POSTGRES_PORT}
    - DB_POSTGRESDB_USER=postgres
    - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
    - DB_POSTGRESDB_DATABASE=${POSTGRES_DB}
    - N8N_DIAGNOSTICS_ENABLED=false
    - N8N_PERSONALIZATION_ENABLED=false
    - N8N_ENCRYPTION_KEY
    - N8N_USER_MANAGEMENT_JWT_SECRET
    - GENERIC_TIMEZONE=America/Phoenix
    - TZ=America/Phoenix


services:
  video-upload-api:
    build:
      context: ./www
      dockerfile: Dockerfile.video-api
    container_name: video-upload-api
    restart: unless-stopped
    expose:
      - 8765
    volumes:
      - /mnt/raven-nas:/mnt/raven-nas
      - ./www/video_upload_api.py:/app/video_upload_api.py:ro
    environment:
      - PYTHONUNBUFFERED=1
    

  nginx:
    image: nginx:latest
    container_name: nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf:rw
      - ./certs:/etc/nginx/certs:ro
      - ./www:/usr/share/nginx/html:rw
      - /mnt/raven-nas/videos-to-process:/data/videos-to-process:ro
    networks:
      - default
      - ai-services-network
    # nginx needs ai-services-network to reach supabase-studio and other cross-project services
    depends_on:
      - video-upload-api
    

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    container_name: open-webui
    expose:
      - 8080
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - AUDIO_TTS_URL=http://kokoro-fastapi-gpu:8880
    

  kokoro-fastapi-gpu:
    build:
      context: ./kokoro-build
      dockerfile: docker/gpu/Dockerfile.rtx5090
      args:
        CUDA_VERSION: "12.8"  # RTX 5090 requires CUDA 12.8 with PyTorch 2.7.1+cu128
    expose:
      - 8880
    restart: always
    volumes:
      # Shared cache volumes - prevent re-downloading models
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      - PYTHONPATH=/app:/app/api
      - USE_GPU=true
      - PYTHONUNBUFFERED=1
      # Cache optimization
      - HF_HOME=/data/.huggingface
      - TORCH_HOME=/data/.torch
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Single RTX 5090
              capabilities:
                - gpu
    

  crawl4ai:
    image: unclecode/crawl4ai:latest # Latest v0.7.6 with integrated GPU support
    # image: unclecode/crawl4ai:gpu-amd64 # Older GPU-specific image (Nov 2024)
    container_name: crawl4ai
    restart: unless-stopped
    # pull_policy: always # Disabled: only pull on explicit rebuild to avoid slow startups
    expose:
      - 8000 # Expose port internally for nginx proxy
    volumes:
      # Shared cache volumes
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      # Define API keys in your .env file or uncomment and set here
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      # Ensure the server runs on 0.0.0.0 to be accessible within the network
      - APP_HOST=0.0.0.0
      - APP_PORT=8000
      - CRAWL4AI_API_KEY=${CRAWL4AI_API_KEY:-}
      # RTX 5090 GPU optimizations
      - MAX_CONCURRENT_TASKS=12  # RTX 5090 optimization: increased from 8 to 12 for better throughput
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      # Cache optimization
      - HF_HOME=/data/.huggingface
      - TORCH_HOME=/data/.torch
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Single RTX 5090
              capabilities: [gpu]
    

  whisperx:
    build:
      context: ./whisperx
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "12.8"  # RTX 5090 requires CUDA 12.8 with PyTorch 2.7.1+cu128
    container_name: whisperx
    restart: unless-stopped
    expose:
      - 8000
    volumes:
      - whisperx-cache:/root/.cache
      - ./shared:/app/shared
      # Mount individual Python files for development (not entire /app to preserve installed packages)
      - ./whisperx/api_server.py:/app/api_server.py
      - ./whisperx/ffmpeg_processor.py:/app/ffmpeg_processor.py
      - ./whisperx/video_segmenter.py:/app/video_segmenter.py
      - /mnt/raven-nas:/mnt/raven-nas
      # Shared cache volumes - prevent re-downloading models
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      - COMPUTE_TYPE=float16
      - BATCH_SIZE=48  # RTX 5090 optimization: increased from 32 to 48 for better GPU utilization
      - HF_TOKEN=${HF_TOKEN:-}
      - LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}
      # Cache optimization - share models across services
      - HF_HOME=/data/.huggingface
      - TRANSFORMERS_CACHE=/data/.huggingface/transformers
      - TORCH_HOME=/data/.torch
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Single RTX 5090
              capabilities: [gpu]
    

  # llama.cpp Chat Service - High-performance LLM inference with multimodal support
  # Built from source with CUDA 12.8 optimizations for RTX 5090
  llama-cpp:
    build:
      context: ./llama-cpp-service
      dockerfile: Dockerfile
    container_name: llama-cpp
    restart: unless-stopped
    expose:
      - 8000
    volumes:
      # Shared cache volumes - models downloaded to HF cache shared across services
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      # Model configuration - uses HuggingFace cache path
      - MODEL_PATH=/data/.huggingface/hub/models--Qwen--Qwen3-VL-30B-A3B-Instruct-GGUF/snapshots/latest/qwen3-vl-30b-a3b-instruct-q4_k_m.gguf
      - HOST=0.0.0.0
      - PORT=8000
      - CONTEXT_SIZE=8192
      - N_GPU_LAYERS=99  # Offload all layers to GPU
      # CUDA configuration for RTX 5090
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      # Cache paths - shared with other services
      - HF_HOME=/data/.huggingface
      - TORCH_HOME=/data/.torch
      # Performance optimizations
      - CUDA_ALLOW_TF32=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Allow time for model loading
    

  # Model Orchestrator - Manages GPU memory and model lifecycle
  # Provides explicit load/unload control for n8n workflows
  model-orchestrator:
    build:
      context: ./model-orchestrator
      dockerfile: Dockerfile
    container_name: model-orchestrator
    restart: unless-stopped
    expose:
      - 8000
    environment:
      - HOST=0.0.0.0
      - PORT=8000
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    

  progress-tracker:
    image: python:3.11-alpine
    container_name: progress-tracker
    restart: unless-stopped
    expose:
      - 5555
    volumes:
      - ./www/progress_tracker.py:/app.py:ro
    command: >
      /bin/sh -c "
        pip install --no-cache-dir flask flask-cors &&
        python /app.py"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5555/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
    

  comfyui:
    build:
      context: ./ComfyUI
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "12.8"
    container_name: comfyui
    restart: unless-stopped
    ports:
      - "18188:18188"  # Expose ComfyUI port to host
    expose:
      - 8188
    volumes:
      - ./shared:/app/shared
      - ./ComfyUI/models:/workspace/ComfyUI/models
      - ./ComfyUI/custom_nodes:/workspace/ComfyUI/custom_nodes
      - ./custom_code/comfyui/workflows:/workspace/ComfyUI/user/default/workflows
      - /mnt/raven-nas/inspirational-shorts/generated-content:/workspace/ComfyUI/output
      - /mnt/raven-nas:/mnt/raven-nas
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
      # Shared Wan models from wan2gp (read-only to avoid conflicts)
      - wan2gp-models:/workspace/shared-wan-models:ro
      # Wan2.2-TI2V-5B models from Ovi (read-only)
      - ./ovi/ckpts/Wan2.2-TI2V-5B:/workspace/ovi-models/Wan2.2-TI2V-5B:ro
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - HF_HOME=/data/.huggingface
      - TORCH_HOME=/data/.torch
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    

  shorts-generator:
    build:
      context: ./custom_code/shorts-generator
      dockerfile: Dockerfile
    container_name: shorts-generator
    restart: unless-stopped
    expose:
      - 7860  # Gradio UI
      - 8000  # FastAPI
    volumes:
      - /mnt/raven-nas:/mnt/raven-nas
      # Mount source code for development (no rebuild needed for code changes)
      - ./custom_code/shorts-generator:/app
      # Mount the container's .venv over the host's broken one
      - shorts-generator-venv:/app/.venv
    environment:
      - N8N_API_URL=http://n8n:5678
      - N8N_WEBHOOK_PATH=/webhook/shorts-generate
      - N8N_API_KEY=${N8N_API_KEY}
      - API_BASE_URL=http://localhost:8000
    depends_on:
      - n8n
      - comfyui
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/api/health').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
    

  n8n-cert-init:
    image: alpine:latest
    container_name: n8n-cert-init
    volumes:
      - ./certs:/certs-src:ro
      - n8n-certs:/certs-dest
    command: sh -c "cp -v /certs-src/* /certs-dest/ && chown -R 1000:1000 /certs-dest && echo 'Certs copied and permissions set.'"
    # This container runs once and exits
    

  n8n:
    <<: *service-n8n
    container_name: n8n
    restart: unless-stopped
    expose:
      - 5678
    volumes:
      - ./n8n-data:/home/node/.n8n  # Bind mount n8n data to custom_code folder
      - ./n8n/backup:/backup
      - ./n8n/shared:/data/shared
      - /mnt/raven-nas:/mnt/raven-nas
      - n8n-certs:/opt/custom-certificates:ro # Mount the prepared named volume
      - ./n8n/custom-nodes:/custom_code/n8n-nodes:ro # Mount custom nodes for auto-installation
    environment:
      - N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE=true
      - NODE_EXTRA_CA_CERTS=/opt/custom-certificates/local-cert.pem
      - WEBHOOK_URL=https://n8n.lan
      - N8N_HOST=n8n.lan
      - N8N_PROTOCOL=https
      - N8N_PORT=5678
      - N8N_EDITOR_BASE_URL=https://n8n.lan
      - N8N_PAYLOAD_SIZE_MAX=2147483648
      - N8N_DEFAULT_BINARY_DATA_MODE=filesystem
      - N8N_FORM_TRIGGER_MAX_PAYLOAD_SIZE=2147483648
      - EXECUTIONS_TIMEOUT=3600
      - EXECUTIONS_TIMEOUT_MAX=7200
    networks:
      - default
      - ai-services-network
    depends_on:
      - n8n-cert-init # Ensure certs are ready before n8n starts
    

    # depends_on:
    #   - n8n-import

  nocodb:
    image: nocodb/nocodb:latest
    container_name: nocodb
    restart: unless-stopped
    expose:
      - 8080
    volumes:
      - nocodb:/usr/app/data/
    environment:
      - NC_DB=pg://${POSTGRES_HOST}:${POSTGRES_PORT}?u=postgres&p=${POSTGRES_PASSWORD}&d=${POSTGRES_DB}
      - NC_AUTH_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
    networks:
      - default
      - ai-services-network
    depends_on:
      - n8n
    

  backup:
    image: offen/docker-volume-backup:v2
    container_name: backup
    restart: unless-stopped
    env_file: .env
    volumes:
      - backup_data:/backup
      - n8n_storage:/backup-src/n8n:ro
      - open-webui:/backup-src/open-webui:ro
      - nocodb:/backup-src/nocodb:ro
    environment:
      - BACKUP_CRON_EXPRESSION=0 0 * * * # Every day at midnight
      - BACKUP_FILENAME=backup-%Y-%m-%d-%H-%M-%S.tar.gz
      - BACKUP_RETENTION_DAYS=7
      - BACKUP_COMPRESSION_LEVEL=9
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - BACKUP_POSTGRES_ENABLED=true
      - BACKUP_STOP_CONTAINERS=false
      - BACKUP_DELETE_ONLY_IF_NEW_BACKUP_PRESENT=true
      - TZ=UTC
    networks:
      - default
      - ai-services-network
    

  service-status:
    image: alpine:latest
    container_name: service-status
    restart: unless-stopped
    volumes:
      - backup_data:/backup:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./scripts/service_status.py:/app.py:ro
    expose:
      - 80
    command: >
      /bin/sh -c "
        apk add --no-cache python3 py3-pip curl jq docker-cli &&
        python3 -m venv /venv &&
        . /venv/bin/activate &&
        pip install --no-cache-dir flask flask-cors &&
        python3 /app.py"
    

  # Docker Registry Pull-Through Cache
  # Caches all pulled Docker images locally to speed up rebuilds
  # Configure Docker daemon to use this cache: /etc/docker/daemon.json
  # { "registry-mirrors": ["http://localhost:5000"] }
  registry-cache:
    image: registry:2
    container_name: registry-cache
    restart: unless-stopped
    ports:
      - "5000:5000"
    environment:
      REGISTRY_PROXY_REMOTEURL: https://registry-1.docker.io
      REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /var/lib/registry
      REGISTRY_STORAGE_DELETE_ENABLED: "true"
    volumes:
      - registry-cache:/var/lib/registry
    

  # Virtual Assistant - Web Interface
  # Uncomment if you need browser-based visual assistant with speech and avatar features
  # virtual-assistant-web:
  #   build:
  #     context: ./virtual-assistant
  #     dockerfile: Dockerfile
  #   container_name: virtual-assistant-web
  #   restart: unless-stopped
  #   expose:
  #     - 8080
  #   volumes:
  #     - ./virtual-assistant:/app
  #     - ./virtual-assistant/shared:/app/shared
  #     # Shared cache volumes
  #     - hf-cache:/data/.huggingface
  #     - torch-cache:/data/.torch
  #   environment:
  #     - RIVA_ASR_URL=riva-asr:50051
  #     - RIVA_TTS_URL=riva-tts:50052
  #     - AUDIO2FACE_URL=http://audio2face:8000
  #     - HF_HOME=/data/.huggingface
  #     - TORCH_HOME=/data/.torch
  #   depends_on:
  #     - riva-asr
  #     - riva-tts
  #     - audio2face
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # NVIDIA Riva ASR - Speech Recognition
  # COMMENTED OUT - Not needed with Ovi and Wan2GP for video/audio generation
  # riva-asr:
  #   image: nvcr.io/nim/nvidia/parakeet-ctc-1.1b-asr:latest
  #   container_name: riva-asr
  #   runtime: nvidia
  #   shm_size: '8gb'
  #   restart: unless-stopped
  #   expose:
  #     - 9000
  #     - 50051
  #   volumes:
  #     - va-riva-cache:/opt/nim/.cache
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=0
  #     - NGC_API_KEY=${NGC_API_KEY}
  #     - NIM_HTTP_API_PORT=9000
  #     - NIM_GRPC_API_PORT=50051
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9000/v1/health/ready"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 600s

  # NVIDIA Riva TTS - Text-to-Speech
  # COMMENTED OUT - Not needed with Ovi and Wan2GP for video/audio generation
  # riva-tts:
  #   image: nvcr.io/nim/nvidia/fastpitch-hifigan-tts:latest
  #   container_name: riva-tts
  #   runtime: nvidia
  #   shm_size: '8gb'
  #   restart: unless-stopped
  #   expose:
  #     - 9001
  #     - 50052
  #   volumes:
  #     - va-riva-tts-cache:/opt/nim/.cache
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=0
  #     - NGC_API_KEY=${NGC_API_KEY}
  #     - NIM_HTTP_API_PORT=9001
  #     - NIM_GRPC_API_PORT=50052
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9001/v1/health/ready"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 600s

  # NVIDIA Audio2Face - Avatar Animation
  # COMMENTED OUT - Not needed with Ovi and Wan2GP for video/audio generation
  # audio2face:
  #   image: nvcr.io/nim/nvidia/audio2face-3d:1.3.16
  #   container_name: audio2face
  #   runtime: nvidia
  #   restart: unless-stopped
  #   expose:
  #     - 8000
  #   networks:
  #     - localai_default
  #   volumes:
  #     - va-audio2face-data:/data
  #     - va-audio2face-models:/models
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=0
  #     - NGC_API_KEY=${NGC_API_KEY}
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health/ready"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  # Ovi - Audio-Video Generation (11B twin backbone model)
  # Generates synchronized video+audio from text or text+image
  # RTX 5090 optimized with CUDA 12.8 and Flash Attention 3
  ovi:
    build:
      context: ./ovi
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "12.8"  # RTX 5090 requires CUDA 12.8
    container_name: ovi
    restart: unless-stopped
    expose:
      - 7860
    volumes:
      # Model storage - use bind mount to access pre-downloaded models
      - ./ovi/ckpts:/workspace/ckpts
      # Output storage - preserves generated videos
      - ./ovi/outputs:/workspace/outputs
      # Shared cache volumes - prevent re-downloading common models
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
      # Mount example prompts for easy access
      - ./ovi/example_prompts:/workspace/example_prompts:ro
      # Mount config for runtime modifications
      - ./ovi/ovi/configs:/workspace/ovi/configs
      # Mount modules for live code updates (development mode)
      - ./ovi/ovi/modules:/workspace/ovi/modules
      # Mount gradio_app.py for live code updates (development mode)
      - ./ovi/gradio_app.py:/workspace/gradio_app.py
    environment:
      # Gradio configuration
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      # CUDA configuration for RTX 5090
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      # Cache paths
      - HF_HOME=/data/.huggingface
      - TRANSFORMERS_CACHE=/data/.huggingface/transformers
      - TORCH_HOME=/data/.torch
      - DIFFUSERS_CACHE=/data/.huggingface/diffusers
      # Performance optimizations for RTX 5090
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - CUDA_MODULE_LOADING=LAZY
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # RTX 5090 32GB VRAM Mode - Use CPU offload + qint8 for Flash Attention fallback compatibility
    # Alternative configs:
    #   Full speed (80GB VRAM): ["python3", "gradio_app.py", "--server_name", "0.0.0.0", "--server_port", "7860"]
    #   Standard 32GB mode: ["python3", "gradio_app.py", "--cpu_offload", "--server_name", "0.0.0.0", "--server_port", "7860"]
    #   FP8 mode (720x720_5s only): ["python3", "gradio_app.py", "--cpu_offload", "--fp8", "--server_name", "0.0.0.0", "--server_port", "7860"]
    command: ["python3", "gradio_app.py", "--cpu_offload", "--qint8", "--server_name", "0.0.0.0", "--server_port", "7860"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Give model loading time to complete
    

  # Ovi API - REST API wrapper for Ovi video+audio generation
  # Provides programmatic access to Ovi 11B for automation and n8n workflows
  # Shares base image and models with Ovi Gradio UI
  ovi-api:
    build:
      context: ./custom_code/ovi-api-server
      dockerfile: Dockerfile
    container_name: ovi-api
    restart: unless-stopped
    expose:
      - 8300
    volumes:
      # Ovi source code - required for imports
      - ./ovi:/workspace/ovi
      # Shared model storage with Ovi Gradio UI
      - ./ovi/ckpts:/workspace/ckpts
      # Output directory for generated videos - unified with ComfyUI
      - /mnt/raven-nas/inspirational-shorts/generated-content:/output
      # Shared NAS storage for cross-container file access
      - /mnt/raven-nas:/mnt/raven-nas
      # Shared cache volumes
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      # Ovi configuration
      - OVI_DIR=/workspace/ovi
      - OVI_CPU_OFFLOAD=true
      - OVI_FP8=false
      - OVI_QINT8=false
      - OVI_OUTPUT_DIR=/output
      # API server configuration
      - PORT=8300
      - HOST=0.0.0.0
      # CUDA configuration for RTX 5090
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      # Cache paths
      - HF_HOME=/data/.huggingface
      - TRANSFORMERS_CACHE=/data/.huggingface/transformers
      - TORCH_HOME=/data/.torch
      - DIFFUSERS_CACHE=/data/.huggingface/diffusers
      # Performance optimizations
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - CUDA_MODULE_LOADING=LAZY
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - ovi  # Ensure base Ovi image is built first
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8300/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s  # Give model loading time (lazy load on first request)
    

  # Wan2GP - Multi-Model Video Generation Platform
  # Supports Wan 2.1/2.2 (1.3B-14B), Hunyuan, LTX, Flux, Qwen
  # Integrated tools: MMAudio, Mask Editor, Queue System, LoRAs
  # RTX 5090 optimized with SageAttention and memory profiles
  wan2gp:
    build:
      context: ./wan2gp
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "12.8"
        CUDA_ARCHITECTURES: "12.0"  # RTX 5090 architecture
    container_name: wan2gp
    restart: unless-stopped
    expose:
      - 7860
    volumes:
      # Model storage - preserves downloaded models (157GB!)
      - wan2gp-models:/workspace/ckpts
      # Output storage - preserves generated videos
      - wan2gp-outputs:/workspace/outputs
      # Settings persistence - user preferences and queue
      - wan2gp-settings:/workspace/settings
      # Config files - bind mount to host for persistence
      - ./wan2gp-config-backup/wgp_config.json:/workspace/wgp_config.json
      - ./wan2gp-config-backup/plugins.json:/workspace/plugins.json
      # LoRA storage - custom model adaptations
      - wan2gp-loras:/workspace/loras
      - wan2gp-loras:/workspace/loras_i2v
      - wan2gp-loras:/workspace/loras_hunyuan
      - wan2gp-loras:/workspace/loras_flux
      - wan2gp-loras:/workspace/loras_ltxv
      - wan2gp-loras:/workspace/loras_qwen
      - wan2gp-loras:/workspace/loras_tts
      # Finetunes storage
      - wan2gp-loras:/workspace/finetunes
      # Shared cache volumes - prevent re-downloading common models
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
      # Mount source for development (optional - comment out for production)
      # - ./wan2gp:/workspace
    environment:
      # Gradio configuration
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      - SERVER_NAME=0.0.0.0  # Required by wgp.py
      # CUDA configuration for RTX 5090
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      # Cache paths
      - HF_HOME=/data/.huggingface
      - TRANSFORMERS_CACHE=/data/.huggingface/transformers
      - TORCH_HOME=/data/.torch
      # Performance optimizations for RTX 5090
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - CUDA_MODULE_LOADING=LAZY
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
      # Run as wanuser
    command: ["python3", "wgp.py"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s  # Give extra time for initial model downloads


  # InfiniteTalk - Audio-Driven Video Generation for Sparse-Frame Video Dubbing
  # Supports unlimited-length talking video with video-to-video and image-to-video modes
  # Synchronizes lips, head movements, body posture, and facial expressions with audio
  infinitetalk:
    build:
      context: ./InfiniteTalk
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "12.8"  # RTX 5090 requires CUDA 12.8
    container_name: infinitetalk
    restart: unless-stopped
    expose:
      - 8418  # Internal port only
    networks:
      - default
    volumes:
      # Model storage - use bind mount to access pre-downloaded models
      - ./InfiniteTalk/weights:/workspace/weights
      # Output storage - preserves generated videos
      - ./InfiniteTalk/outputs:/workspace/outputs
      # Audio cache
      - ./InfiniteTalk/outputs:/workspace/save_audio
      # Example files for Gradio UI
      - ./InfiniteTalk/examples:/workspace/examples
      # Shared cache volumes - prevent re-downloading common models
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
      # Mount source for development (optional)
      # - ./InfiniteTalk:/workspace
    environment:
      # Service configuration
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=8418
      # CUDA configuration for RTX 5090
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      # Cache paths
      - HF_HOME=/data/.huggingface
      - TRANSFORMERS_CACHE=/data/.huggingface/transformers
      - TORCH_HOME=/data/.torch
      # Performance optimizations for RTX 5090
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - CUDA_MODULE_LOADING=LAZY
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: ["python3", "app.py", "--ckpt_dir", "weights/Wan2.1-I2V-14B-480P", "--wav2vec_dir", "weights/chinese-wav2vec2-base", "--infinitetalk_dir", "weights/InfiniteTalk/single/infinitetalk.safetensors", "--num_persistent_param_in_dit", "0", "--motion_frame", "9"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8418/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s  # Give model loading time (5 minutes)
    

  # InfiniteTalk API - FastAPI wrapper for automated video generation (Step 4)
  # Provides REST API for n8n workflow integration
  infinitetalk-api:
    build:
      context: ./infinitetalk-api-server
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "12.8"
    image: infinitetalk-api:latest
    container_name: infinitetalk-api
    restart: unless-stopped
    # GPU optimization: use host IPC for efficient inter-process communication
    ipc: host
    # GPU optimization: increase shared memory for large model inference
    shm_size: '8gb'
    # GPU optimization: remove memory limits for better performance
    ulimits:
      memlock: -1  # unlimited lockable memory
      stack: 67108864  # 64MB stack size
    expose:
      - 8200  # FastAPI backend
    networks:
      - default
    volumes:
      # Shared files with other services
      - ./shared:/data/shared
      # Output storage - preserves generated videos
      - ./infinitetalk-api-server/output:/output
      # Mount InfiniteTalk source code (read-only to avoid conflicts)
      # This provides access to wan modules and core generation logic
      - ./InfiniteTalk:/workspace:ro
      - /mnt/raven-nas:/mnt/raven-nas
      # Shared cache volumes - prevent re-downloading models
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      # Service configuration
      - HOST=0.0.0.0
      - PORT=8200
      - PYTHONUNBUFFERED=1
      # Model paths (from mounted InfiniteTalk directory)
      - CKPT_DIR=/workspace/weights/Wan2.1-I2V-14B-480P
      - WAV2VEC_DIR=/workspace/weights/chinese-wav2vec2-base
      - INFINITETALK_DIR=/workspace/weights/InfiniteTalk/single/infinitetalk.safetensors
      # VRAM management (full GPU mode for RTX 5090 32GB - keep all params in VRAM)
      - NUM_PERSISTENT_PARAM_IN_DIT=999999
      - MOTION_FRAME=9
      - TASK=infinitetalk-14B
      - FRAME_NUM=81
      - MODE=streaming
      # CUDA configuration for RTX 5090
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      # Cache optimization - shared across all AI services
      - HF_HOME=/data/.huggingface
      - TRANSFORMERS_CACHE=/data/.huggingface/transformers
      - TORCH_HOME=/data/.torch
      # Performance optimizations for RTX 5090
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - CUDA_MODULE_LOADING=LAZY
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8200/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Quick startup (models load on first request)
    

  # YouTube Tools - YouTube Tools with WhisperX transcription
  # Download audio segments from YouTube videos with automated transcription
  yttools:
    build:
      context: ./yttools
      dockerfile: Dockerfile
    container_name: yttools
    restart: unless-stopped
    expose:
      - 7860  # Gradio UI
      - 8456  # FastAPI backend
    networks:
      - default
    volumes:
      # Source code - mount for live development (comment out for production)
      - ./yttools:/app
      # Mount ai-data for persistent storage
      - /mnt/ai-data:/mnt/ai-data
      # Shared cache volumes - prevent re-downloading models
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      # Service configuration
      - WHISPERX_URL=http://whisperx:8000
      - API_BASE_URL=http://localhost:8456
      - PYTHONUNBUFFERED=1
      # Storage paths - use mounted ai-data directory
      - DOWNLOADS_DIR=/mnt/ai-data/yttools/downloads
      - TASK_STORAGE_DIR=/mnt/ai-data/yttools/task_storage
      # Cache optimization - share models across services
      - HF_HOME=/data/.huggingface
      - TORCH_HOME=/data/.torch
    depends_on:
      - whisperx
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8456/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    

  # VibeVoice API - Microsoft VibeVoice TTS API Server
  # REST API for voice cloning and multi-speaker text-to-speech
  vibevoice-api:
    build:
      context: ./vibevoice-api-server
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "12.8"  # RTX 5090 requires CUDA 12.8
    container_name: vibevoice-api
    restart: unless-stopped
    expose:
      - 8100  # FastAPI backend
    networks:
      - default
    volumes:
      # Bind mount VibeVoice models directory directly from host
      - ./ComfyUI/models/vibevoice:/workspace/ComfyUI/models/vibevoice
      # Output storage - preserves generated audio files
      - ./vibevoice-api-server/output:/output
      - /mnt/raven-nas:/mnt/raven-nas
      # Shared cache volumes - prevent re-downloading models
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      # Service configuration
      - MODELS_DIR=/workspace/ComfyUI/models/vibevoice
      - HOST=0.0.0.0
      - PORT=8100
      - DEFAULT_MODEL=VibeVoice-Large
      - ENABLE_CORS=true
      - PYTHONUNBUFFERED=1
      # CUDA configuration for RTX 5090
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      # Cache optimization - share models across services
      - HF_HOME=/data/.huggingface
      - TRANSFORMERS_CACHE=/data/.huggingface/transformers
      - TORCH_HOME=/data/.torch
      # Performance optimizations for RTX 5090
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - CUDA_MODULE_LOADING=LAZY
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8100/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Give model loading time to complete
    

  # Emu3 - Multimodal Image Generation with Emu3.5
  # Dual-mode service with Gradio UI and FastAPI endpoints
  # Supports text-to-image, image understanding, and story generation
  # emu3:
  #   build:
  #     context: ./emu3
  #     dockerfile: Dockerfile
  #     args:
  #       CUDA_VERSION: "12.8"  # RTX 5090 requires CUDA 12.8
  #       # No CUDA_ARCHITECTURES needed - using prebuilt wheels to avoid WSL crashes
  #   container_name: emu3
  #   restart: unless-stopped
  #   expose:
  #     - 8200  # Gradio UI + FastAPI backend
  #   networks:
  #     - default
  #   volumes:
  #     # Named volume for model persistence
  #     - emu3-models:/app/models
  #     # Bind mount for generated images
  #     - ./emu3/outputs:/app/outputs
  #     # Shared cache volumes - prevent re-downloading models
  #     - hf-cache:/data/.huggingface
  #     - torch-cache:/data/.torch
  #   environment:
  #     # Service configuration
  #     - GRADIO_SERVER_NAME=0.0.0.0
  #     - GRADIO_SERVER_PORT=8200
  #     - PYTHONUNBUFFERED=1
  #     # CUDA configuration for RTX 5090
  #     - CUDA_VISIBLE_DEVICES=0
  #     - NVIDIA_VISIBLE_DEVICES=0
  #     # Cache optimization - share models across services
  #     - HF_HOME=/data/.huggingface
  #     - TRANSFORMERS_CACHE=/data/.huggingface/transformers
  #     - TORCH_HOME=/data/.torch
  #     # Performance optimizations for RTX 5090
  #     - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
  #     - CUDA_MODULE_LOADING=LAZY
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8200/api/v1/health', timeout=5)"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 180s  # Extended startup time for model loading

  peggy-chatbot:
    build:
      context: ./custom_code/peggy_oliveira_chatbot/chatbot-api
      dockerfile: Dockerfile
    container_name: peggy-chatbot
    restart: unless-stopped
    expose:
      - 8989
    volumes:
      # Shared cache volumes - prevent re-downloading models and packages
      - hf-cache:/root/.cache/huggingface
      - torch-cache:/root/.cache/torch
      - pip-cache:/root/.cache/pip
    environment:
      - PYTHONUNBUFFERED=1
      - SUPABASE_URL=https://supabase.lan
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY:-}
      - LM_STUDIO_URL=https://lmstudio.lan
      - LM_STUDIO_MODEL=qwen/qwen3-vl-30b
      - API_PORT=8989
      # Cache optimization - share models across services
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/transformers
      - TORCH_HOME=/root/.cache/torch
      - SENTENCE_TRANSFORMERS_HOME=/root/.cache/huggingface/sentence-transformers
    networks:
      - ai-services-network
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8989/', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  peggy-chatbot-ui:
    build:
      context: ./custom_code/peggy_oliveira_chatbot/chatbot-ui
      dockerfile: Dockerfile
    container_name: peggy-chatbot-ui
    restart: unless-stopped
    expose:
      - 3000
    environment:
      - NEXT_PUBLIC_API_URL=http://peggy-chatbot:8989
      - BACKEND_URL=http://peggy-chatbot:8989
    networks:
      - ai-services-network
    depends_on:
      - peggy-chatbot
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

# Networks configuration - shared network for cross-project communication
# Services join ai-services-network to communicate with services in other compose projects
networks:
  ai-services-network:
    name: ai-services-network
    external: true
